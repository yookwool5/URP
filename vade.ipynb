{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c61ac9d9-bb28-4a58-92d1-91affb9d65da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"An implementation of VaDE(https://arxiv.org/pdf/1611.05148.pdf).\n",
    "\"\"\"\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "def _reparameterize(mu, logvar):\n",
    "    \"\"\"Reparameterization trick.\n",
    "    \"\"\"\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    z = mu + eps * std\n",
    "    return z\n",
    "\n",
    "\n",
    "class VaDE(torch.nn.Module):\n",
    "    \"\"\"Variational Deep Embedding(VaDE).\n",
    "\n",
    "    Args:\n",
    "        n_classes (int): Number of clusters.\n",
    "        data_dim (int): Dimension of observed data.\n",
    "        latent_dim (int): Dimension of latent space.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes, data_dim, latent_dim):\n",
    "        super(VaDE, self).__init__()\n",
    "\n",
    "        self._pi = Parameter(torch.zeros(n_classes))\n",
    "        self.mu = Parameter(torch.randn(n_classes, latent_dim))\n",
    "        self.logvar = Parameter(torch.randn(n_classes, latent_dim))\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(data_dim, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 2048),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.encoder_mu = torch.nn.Linear(2048, latent_dim)\n",
    "        self.encoder_logvar = torch.nn.Linear(2048, latent_dim)\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(latent_dim, 2048),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(2048, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, data_dim),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        return torch.softmax(self._pi, dim=0)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.encoder_mu(h)\n",
    "        logvar = self.encoder_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = _reparameterize(mu, logvar)\n",
    "        recon_x = self.decode(z)\n",
    "        return recon_x, mu, logvar\n",
    "\n",
    "    def classify(self, x, n_samples=8):\n",
    "        with torch.no_grad():\n",
    "            mu, logvar = self.encode(x)\n",
    "            z = torch.stack(\n",
    "                [_reparameterize(mu, logvar) for _ in range(n_samples)], dim=1)\n",
    "            z = z.unsqueeze(2)\n",
    "            h = z - self.mu\n",
    "            h = torch.exp(-0.5 * torch.sum(h * h / self.logvar.exp(), dim=3))\n",
    "            # Same as `torch.sqrt(torch.prod(self.logvar.exp(), dim=1))`\n",
    "            h = h / torch.sum(0.5 * self.logvar, dim=1).exp()\n",
    "            p_z_given_c = h / (2 * math.pi)\n",
    "            p_z_c = p_z_given_c * self.weights\n",
    "            y = p_z_c / torch.sum(p_z_c, dim=2, keepdim=True)\n",
    "            y = torch.sum(y, dim=1)\n",
    "            pred = torch.argmax(y, dim=1)\n",
    "        return pred\n",
    "\n",
    "\n",
    "def lossfun(model, x, recon_x, mu, logvar):\n",
    "    batch_size = x.size(0)\n",
    "\n",
    "    # Compute gamma ( q(c|x) )\n",
    "    z = _reparameterize(mu, logvar).unsqueeze(1)\n",
    "    h = z - model.mu\n",
    "    h = torch.exp(-0.5 * torch.sum((h * h / model.logvar.exp()), dim=2))\n",
    "    # Same as `torch.sqrt(torch.prod(model.logvar.exp(), dim=1))`\n",
    "    h = h / torch.sum(0.5 * model.logvar, dim=1).exp()\n",
    "    p_z_given_c = h / (2 * math.pi)\n",
    "    p_z_c = p_z_given_c * model.weights\n",
    "    gamma = p_z_c / torch.sum(p_z_c, dim=1, keepdim=True)\n",
    "\n",
    "    h = logvar.exp().unsqueeze(1) + (mu.unsqueeze(1) - model.mu).pow(2)\n",
    "    h = torch.sum(model.logvar + h / model.logvar.exp(), dim=2)\n",
    "    loss = F.binary_cross_entropy(recon_x, x, reduction='sum') \\\n",
    "        + 0.5 * torch.sum(gamma * h) \\\n",
    "        - torch.sum(gamma * torch.log(model.weights + 1e-9)) \\\n",
    "        + torch.sum(gamma * torch.log(gamma + 1e-9)) \\\n",
    "        - 0.5 * torch.sum(1 + logvar)\n",
    "\n",
    "    loss = loss / batch_size\n",
    "    return loss\n",
    "\n",
    "def ordloss(model, x, data_dim, latent_dim):\n",
    "    loss=0\n",
    "    relu = torch.nn.ReLU()\n",
    "    for i in range (data_dim - 1) :\n",
    "        for j in range (latent_dim - 1):\n",
    "            loss += np.exp(model.mu[i,j] - model.mu[i+1,j])\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "class AutoEncoderForPretrain(torch.nn.Module):\n",
    "    \"\"\"Auto-Encoder for pretraining VaDE.\n",
    "\n",
    "    Args:\n",
    "        data_dim (int): Dimension of observed data.\n",
    "        latent_dim (int): Dimension of latent space.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dim, latent_dim):\n",
    "        super(AutoEncoderForPretrain, self).__init__()\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(data_dim, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 2048),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.encoder_mu = torch.nn.Linear(2048, latent_dim)\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(latent_dim, 2048),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(2048, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, data_dim),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder_mu(self.encoder(x))\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        recon_x = self.decode(z)\n",
    "        return recon_x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
